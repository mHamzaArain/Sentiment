{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f52e5a0-46b2-4d22-9774-94c513983778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d688fe41-d89d-4ff8-b94d-fc08ea8012c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11109, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = joblib.load('/home/hamza/Documents/sentiments/data/01_processed_data.joblib')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d7a39e-597c-4ff8-b1d6-e78f291b99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upcasting\n",
    "def augmentMyData(df, augmenter, label, repetitions=1, samples=200):\n",
    "    from sklearn.utils import shuffle\n",
    "    augmented_texts = []\n",
    "    # select only the minority class samples\n",
    "    spam_df = df[df['Sentiment'] == label].reset_index(drop=True) # removes unecessary index column\n",
    "    for i in tqdm(np.random.randint(0, len(spam_df), samples)):\n",
    "        # generating 'n_samples' augmented texts\n",
    "        for _ in range(repetitions):\n",
    "            augmented_text = augmenter.augment(spam_df['processed_text'].iloc[i])\n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    data = {\n",
    "        'Sentiment': label,\n",
    "        'processed_text': augmented_texts\n",
    "    }\n",
    "    \n",
    "    aug_df = pd.DataFrame(data)\n",
    "    dff = shuffle(df.append(aug_df).reset_index(drop=True))\n",
    "    return dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bafdf35-f2e5-4f8f-bc74-cb195f71aaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f763c621018d41daa88189c1563215d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beadb36f3c54dc68914b63b9873ac3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "\n",
    "augmenter = aug.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
    "aug = augmentMyData(df, augmenter, samples=1000, label=0)\n",
    "aug_df = augmentMyData(aug, augmenter, samples=1500, label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64df0c9d-3199-4302-9d18-5f8b35652412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    4000\n",
       "0    3753\n",
       "1    3169\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = aug_df[aug_df['Sentiment']==0]\n",
    "df2 = aug_df[aug_df['Sentiment']==1]\n",
    "df4 = aug_df[aug_df['Sentiment']==2][:4000]\n",
    "\n",
    "aug_df = pd.concat([df1, df2,df4])# df3, df4])\n",
    "\n",
    "aug_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9398da-5a36-42f6-aad7-eeab457adb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=2000)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_train = X_train.toarray()\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    X_test = X_test.toarray()\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f936c6-8962-422c-a501-7e4f17e3f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dfff, ratio):\n",
    "    Y = dfff['Sentiment'].values\n",
    "    X = dfff['processed_text']\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Random sampling\n",
    "    return train_test_split(X, Y, test_size=ratio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac0235d-65f6-4830-93c2-204b62abe224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BestMLAlgo(nlp_model, x_train, x_test, y_train, y_test):\n",
    "    \"\"\"Identify best Algo on given dataset\"\"\"\n",
    "    from prettytable import PrettyTable\n",
    "    \n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Vect\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1Score\", \"Log loss\", \"Roc Auc\"]\n",
    "                        \n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    import xgboost\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import precision_score, accuracy_score, roc_auc_score,\\\n",
    "                                roc_curve, auc, log_loss, recall_score\n",
    "    \n",
    "    base_models = {\n",
    "        'kNN': KNeighborsClassifier(),\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Log. Reg.': LogisticRegression(),\n",
    "        'SVM Linear': SGDClassifier(class_weight='balanced', penalty='l2', loss='hinge', random_state=42),\n",
    "        'SVM Non-linear': svm.SVC(kernel='rbf'),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boost': GradientBoostingClassifier(),\n",
    "        'Ada Boost': AdaBoostClassifier(),\n",
    "        'xgboost': xgboost.XGBClassifier(),\n",
    "    }          \n",
    "        \n",
    "    for model_name, model in base_models.items():\n",
    "        model.fit(x_train, y_train)\n",
    "        model = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred_proba = model.predict_proba(x_test)\n",
    "        y_pred = model.predict(x_test)\n",
    "            \n",
    "        # Performance metrics\n",
    "        accuracy        = round(accuracy_score(y_test, y_pred), 2)\n",
    "        precision       = round(precision_score(y_test, y_pred, average='micro'), 2)\n",
    "        recall          = round(recall_score(y_test, y_pred, average='micro'), 2)\n",
    "        f1_score        = round((2*recall*precision)/(recall+precision), 2)\n",
    "        loss            = round(log_loss(y_test, y_pred_proba, eps=1e-15), 2) # , labels=model.classes\n",
    "        roc_auc         = round(roc_auc_score(y_test, y_pred_proba, multi_class=\"ovr\"), 2)        \n",
    "        \n",
    "        table.add_row([nlp_model, model_name, accuracy, precision, recall, f1_score, loss, roc_auc])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f7f6e2-d95f-4236-8afd-9a3caedfef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10902, 2000)\n",
      "(20, 2000)\n",
      "(10902, 2000)   (20, 2000)   (10902,)   (20,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(aug_df, ratio=20)\n",
    "X_train, X_test, VEC = tfidf(X_train, X_test)\n",
    "\n",
    "\n",
    "print(X_train.shape, \" \", X_test.shape, \" \", y_train.shape, \" \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60b8ef2-7a6d-4f67-bfcb-330819243817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:40] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:44:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:46:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:47:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:49:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:51:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "+-------+----------------+----------+-----------+--------+---------+----------+---------+\n",
      "|  Vect |     Model      | Accuracy | Precision | Recall | F1Score | Log loss | Roc Auc |\n",
      "+-------+----------------+----------+-----------+--------+---------+----------+---------+\n",
      "| TFIDF |      kNN       |   0.75   |    0.75   |  0.75  |   0.75  |   0.64   |   0.9   |\n",
      "| TFIDF |  Naive Bayes   |   0.7    |    0.7    |  0.7   |   0.7   |   0.55   |   0.93  |\n",
      "| TFIDF |   Log. Reg.    |   0.65   |    0.65   |  0.65  |   0.65  |   0.6    |   0.9   |\n",
      "| TFIDF |   SVM Linear   |   0.65   |    0.65   |  0.65  |   0.65  |   0.6    |   0.91  |\n",
      "| TFIDF | SVM Non-linear |   0.75   |    0.75   |  0.75  |   0.75  |   0.62   |   0.91  |\n",
      "| TFIDF | Decision Tree  |   0.8    |    0.8    |  0.8   |   0.8   |   0.61   |   0.97  |\n",
      "| TFIDF | Random Forest  |   0.85   |    0.85   |  0.85  |   0.85  |   0.47   |   0.98  |\n",
      "| TFIDF | Gradient Boost |   0.7    |    0.7    |  0.7   |   0.7   |   0.73   |   0.91  |\n",
      "| TFIDF |   Ada Boost    |   0.7    |    0.7    |  0.7   |   0.7   |   0.9    |   0.82  |\n",
      "| TFIDF |    xgboost     |   0.8    |    0.8    |  0.8   |   0.8   |   0.55   |   0.92  |\n",
      "+-------+----------------+----------+-----------+--------+---------+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "BestMLAlgo(\"TFIDF\", X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d254839-b9ec-4e5b-a55e-f1e258b43c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/oversampled.df']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(aug_df, \"data/oversampled.df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff0cbd24-0406-44c7-b2b4-be0c02a2a205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    4000\n",
       "0    3753\n",
       "1    3169\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750c0dd-c6d4-47ae-b79c-a417073ac773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
