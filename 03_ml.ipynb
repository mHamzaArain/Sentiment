{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0426f833-e8f4-425e-8c89-fbcadd04dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f790f9-9c71-46af-a661-9586389ec187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(X_train, X_test):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    # from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=2000, ngram=(1, 4))\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_train = X_train.toarray()\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    X_test = X_test.toarray()\n",
    "    print(X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d127d9-f876-4553-884b-811151d0d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dfff, ratio):\n",
    "    Y = dfff['Sentiment'].values\n",
    "    X = dfff['processed_text']\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Random sampling\n",
    "    return train_test_split(X, Y, test_size=ratio) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8d4d66-ab84-4979-83d3-b8f2b17e4611",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM Linear & Non Linear\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc \\\n",
    "                , accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sgd_svm_classifier(X_train, y_train, X_test, y_test, kernel=None):\n",
    "    if kernel == None:\n",
    "        alphaList = np.array([0.0001,])\n",
    "        params_dict = [{'alpha': alphaList}]\n",
    "        svc_or_sgd = SGDClassifier(loss='hinge', penalty='l2',n_jobs=-1)\n",
    "        # print(svc_or_sgd)\n",
    "    \n",
    "    \n",
    "    elif kernel == \"rbf\":\n",
    "        cList = [0.001,]\n",
    "        gammaList = [1, 10]\n",
    "        params_dict = [{'C': cList, 'gamma': gammaList}]\n",
    "        svc_or_sgd = SVC(kernel='rbf')\n",
    "        # print(svc_or_sgd)\n",
    "        \n",
    "    grid = GridSearchCV(estimator=svc_or_sgd, \n",
    "                    param_grid=params_dict, \n",
    "                    scoring='f1_micro', n_jobs=3,\n",
    "                    cv=3, return_train_score=True)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    if kernel == None:\n",
    "        # Model trainig\n",
    "        model = SGDClassifier(loss='hinge', alpha=grid_result.best_estimator_.get_params()['alpha'],\n",
    "                                    penalty='l2', \n",
    "                                    n_jobs=-1)\n",
    "        \n",
    "    elif kernel == \"rbf\":\n",
    "        model = SVC(kernel='rbf', C=grid_result.best_estimator_.get_params()['C'],\n",
    "                    gamma=grid_result.best_estimator_.get_params()['gamma'],\n",
    "                    probability=True)\n",
    "        # we have to fit the SGDClassifier so that we can access the coef_\n",
    "        \n",
    "    model = model.fit(X_train, y_train)\n",
    "    calibrator = CalibratedClassifierCV(model, cv=5, \n",
    "                                        method='isotonic')\n",
    "    # Lets refit the calibrator to find probabilities\n",
    "    calibrator.fit(X_train, y_train)\n",
    "    y_pred = calibrator.predict(X_test)\n",
    "\n",
    "    print(\"Optimal Parameters : \", grid_result.best_estimator_.get_params())\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, y_pred))\n",
    "      \n",
    "    return calibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14dcfaf-135e-461b-aa02-2aa6faea6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive bayes\n",
    "\n",
    "def nb_classifier(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    This method is a wrapper over the actual naive bayes classifier. It will return the most optimal value \n",
    "    of Alpha based on the results obtained in cross_validation after running the algorithm on the given dataset.\n",
    "    \"\"\"\n",
    "    alphaList = np.array([0.5,])\n",
    "    params_Dict = {'alpha' : alphaList}\n",
    "    nb_Optimal = MultinomialNB()\n",
    "    grid = GridSearchCV(estimator=nb_Optimal, \n",
    "                        param_grid=params_Dict,\n",
    "                        scoring='f1_micro', n_jobs=4, cv=5, return_train_score=True)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    print(\"Optimal Parameters : \", grid_result.best_estimator_.get_params())\n",
    "\n",
    "    model = MultinomialNB(alpha=grid_result.best_estimator_.get_params()['alpha'])\n",
    "    model.fit(X_train, y_train)\n",
    "    model = CalibratedClassifierCV(model, cv=5, \n",
    "                                        method='isotonic')\n",
    "    # Lets refit the calibrator to find probabilities\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # returning the optimal value of K using the MSE of cross validation scores\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9e29f6-a745-4ef1-85a3-76a69a5deb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN\n",
    "\n",
    "def knn_classifier(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    This method run the kNN classification algorithm on the given dataset, using brute force approach. \n",
    "    It plots the curves of various metrics employed to gauge the classifier performance, and returns the \n",
    "    optimal value of k, based on the cross_validaton_score as defined in the problem\n",
    "    '''\n",
    "\n",
    "    # knn_classifier(X_train, y_train, X_test, y_test)\n",
    "    kNeighbours = [1,]    \n",
    "    algo = ['ball_tree']\n",
    "    params_dict = [{'n_neighbors': kNeighbours, 'algorithm':algo}]\n",
    "    knn = KNeighborsClassifier()\n",
    "\n",
    "    grid = GridSearchCV(estimator=knn, \n",
    "                param_grid=params_dict, \n",
    "                scoring='f1_micro', n_jobs=3,\n",
    "                cv=3, return_train_score=True)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    print(\"Optimal Parameters : \", grid_result.best_estimator_.get_params())\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=grid_result.best_estimator_.get_params()['n_neighbors'],\n",
    "                                       algorithm=grid_result.best_estimator_.get_params()['algorithm'])\n",
    "    model.fit(X_train, y_train)\n",
    "    calibrator = CalibratedClassifierCV(model, cv=5, \n",
    "                                        method='isotonic')\n",
    "    # Lets refit the calibrator to find probabilities\n",
    "    calibrator.fit(X_train, y_train)\n",
    "    y_pred = calibrator.predict(X_test)\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # returning the optimal value of K using the MSE of cross validation scores\n",
    "    return calibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a451b92-da86-4d74-b0b5-3dfead96b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logestic Regression\n",
    "\n",
    "def lr_classifier(X_train, y_train, X_test, y_test):\n",
    "    cList = np.array([10])\n",
    "    p = ['l2']\n",
    "    sol = ['liblinear']                \n",
    "    params_dict = [{'C': cList, 'penalty':p, 'solver':sol}]\n",
    "    lr_optimal = LogisticRegression()\n",
    "    \n",
    "    grid = GridSearchCV(estimator=lr_optimal, \n",
    "                        param_grid=params_dict,\n",
    "#                           param_distributions=params_dict, \n",
    "                        scoring='f1_micro', n_jobs=-1, cv=2,  \n",
    "                        return_train_score=True\n",
    "                       )\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    print(\"Optimal Parameters : \", grid_result.best_estimator_.get_params())\n",
    "    \n",
    "    model = LogisticRegression(C=grid_result.best_estimator_.get_params()['C'],\n",
    "                                    penalty=grid_result.best_estimator_.get_params()['penalty'],\n",
    "                                    solver=grid_result.best_estimator_.get_params()['solver'])\n",
    "    model.fit(X_train, y_train)\n",
    "                      \n",
    "    model.fit(X_train, y_train)\n",
    "    model = CalibratedClassifierCV(model, cv=5, \n",
    "                                        method='isotonic')\n",
    "    # Lets refit the calibrator to find probabilities\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # returning the optimal value of K using the MSE of cross validation scores\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6c08aa-dbdd-4993-8373-3985289c903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dicission Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def dt_classifier(X_train, y_train, X_test, y_test):\n",
    "    max_depth = np.array([500, 1000])\n",
    "    min_samples_split = np.array([5,])#10, 100, 500])\n",
    "    params_dict = [{'max_depth': max_depth, \n",
    "                    'min_samples_split': min_samples_split}]\n",
    "    dt_optimal = DecisionTreeClassifier(random_state=1)\n",
    "    \n",
    "    grid = GridSearchCV(estimator=dt_optimal, \n",
    "                        param_grid=params_dict, \n",
    "                        scoring='f1_micro', n_jobs=4, cv=5,\n",
    "                        return_train_score=True)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    print(\"Optimal Parameters : \", grid_result.best_estimator_.get_params())\n",
    "    \n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth=grid_result.best_estimator_.get_params()['max_depth'], \n",
    "                        min_samples_split=grid_result.best_estimator_.get_params()['min_samples_split'],\n",
    "                                random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    model = CalibratedClassifierCV(model, cv=5, \n",
    "                                        method='isotonic')\n",
    "    # Lets refit the calibrator to find probabilities\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # returning the optimal value of K using the MSE of cross validation scores\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8f0b19-017e-4edc-925e-caa5a2fe771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rf_classifier(X_train, y_train, X_test, y_test):\n",
    "    max_depth = np.array([150, 200,])\n",
    "    min_samples_leaf = np.array([1, 2,])\n",
    "    n_estimators = np.array([500, 600])\n",
    "    params_dict = [{'max_depth': max_depth,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'n_estimators': n_estimators}]\n",
    "    rf_optimal = RandomForestClassifier(random_state=1,n_jobs=6,\n",
    "                                        class_weight='balanced')\n",
    "    \n",
    "    grid = GridSearchCV(estimator=rf_optimal, \n",
    "                        param_grid=params_dict, \n",
    "                        scoring='f1_micro', n_jobs=4, cv=5,\n",
    "                        return_train_score=True)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    print(\"Optimal Parameters : \", grid_result.best_estimator_.get_params())\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth=grid_result.best_estimator_.get_params()['max_depth'], \n",
    "                        min_samples_split=grid_result.best_estimator_.get_params()['min_samples_split'],\n",
    "                                random_state=1)\n",
    "    \n",
    "    model = RandomForestClassifier(max_depth=grid_result.best_estimator_.get_params()['max_depth'],\n",
    "                        min_samples_leaf=grid_result.best_estimator_.get_params()['min_samples_leaf'],\n",
    "                        n_estimators=grid_result.best_estimator_.get_params()['n_estimators'],  \n",
    "                        n_jobs=-4,\n",
    "                        random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    model = CalibratedClassifierCV(model, cv=5, \n",
    "                                        method='isotonic')\n",
    "    # Lets refit the calibrator to find probabilities\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # returning the optimal value of K using the MSE of cross validation scores\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332ea841-26fd-49c7-b5e2-a863863e11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    4000\n",
       "0    3753\n",
       "1    3169\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = joblib.load('data/oversampled.df')\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fae45ed-0008-46e0-af12-ddaf95ea80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10902, 2000)\n",
      "(20, 2000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_X_train.model']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df, ratio=20)\n",
    "X_train, X_test, VEC = tfidf(X_train, X_test)\n",
    "\n",
    "\n",
    "\n",
    "joblib.dump(VEC, 'tfidf_X_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49bff917-278c-48d7-b4bf-b02c5f06bf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'alpha': 0.0001, 'average': False, 'class_weight': None, 'early_stopping': False, 'epsilon': 0.1, 'eta0': 0.0, 'fit_intercept': True, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'loss': 'hinge', 'max_iter': 1000, 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78         9\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       0.67      0.67      0.67         6\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.81      0.81      0.81        20\n",
      "weighted avg       0.80      0.80      0.80        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/svm_linear.model']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Linear\n",
    "model = sgd_svm_classifier(X_train, y_train, X_test, y_test, kernel=None)\n",
    "joblib.dump(model, \"data/svm_linear.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "382f26b3-c9f9-4943-922a-0bb873df5049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'C': 0.001, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80         8\n",
      "           1       0.78      0.78      0.78         9\n",
      "           2       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.54      0.51      0.53        20\n",
      "weighted avg       0.69      0.65      0.67        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/svm_nonLinear.model']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM Non-Linear\n",
    "model = sgd_svm_classifier(X_train, y_train, X_test, y_test, kernel='rbf')\n",
    "joblib.dump(model, \"data/svm_nonLinear.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91252b9c-27e1-4864-bdc6-1463f5258cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'alpha': 0.5, 'class_prior': None, 'fit_prior': True}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86        10\n",
      "           1       1.00      0.75      0.86         8\n",
      "           2       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.72      0.72      0.70        20\n",
      "weighted avg       0.84      0.80      0.81        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/nb.model']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Niave Bayes\n",
    "model = nb_classifier(X_train, y_train, X_test, y_test)\n",
    "joblib.dump(model, \"data/nb.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06a1b123-8d8d-40d7-ae42-cfda3c387a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'algorithm': 'ball_tree', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 1, 'p': 2, 'weights': 'uniform'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76        10\n",
      "           1       0.86      0.75      0.80         8\n",
      "           2       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.70        20\n",
      "   macro avg       0.53      0.52      0.52        20\n",
      "weighted avg       0.71      0.70      0.70        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/knn.model']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## KNN\n",
    "model = knn_classifier(X_train, y_train, X_test, y_test)\n",
    "joblib.dump(model, \"data/knn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a6dc567-c3f1-4dc6-a3b7-663be264dbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/hamza/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'C': 10.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        10\n",
      "           1       1.00      0.75      0.86         8\n",
      "           2       0.25      0.50      0.33         2\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.72      0.72      0.70        20\n",
      "weighted avg       0.88      0.80      0.83        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/lr.model']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic Regression\n",
    "model = lr_classifier(X_train, y_train, X_test, y_test)\n",
    "joblib.dump(model, \"data/lr.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "265feead-2a83-43ab-8f7e-bec3f11ac529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 500, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78        10\n",
      "           1       0.67      0.50      0.57         8\n",
      "           2       0.17      0.50      0.25         2\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.57      0.57      0.53        20\n",
      "weighted avg       0.72      0.60      0.64        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/dt.model']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Decision Tree\n",
    "model = dt_classifier(X_train, y_train, X_test, y_test)\n",
    "joblib.dump(model, \"data/dt.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ebe2ea5-b743-46aa-99c9-4544f4b9beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Parameters :  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 200, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 600, 'n_jobs': 6, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.75      0.86         8\n",
      "           1       0.88      0.78      0.82         9\n",
      "           2       0.33      0.67      0.44         3\n",
      "\n",
      "    accuracy                           0.75        20\n",
      "   macro avg       0.74      0.73      0.71        20\n",
      "weighted avg       0.84      0.75      0.78        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/rf.model']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Random Forest\n",
    "model = rf_classifier(X_train, y_train, X_test, y_test)\n",
    "joblib.dump(model, \"data/rf.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80866900-26cf-4b36-90ee-027ed4e35562",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stacking\n",
    "\n",
    "def stacking_models(x_train, y_train, x_test, y_test, x_cv, y_cv, *models):\n",
    "    from sklearn.metrics import log_loss,f1_score,  classification_report\n",
    "    from mlxtend.classifier import StackingClassifier\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "    models_list = []\n",
    "    for model in models:\n",
    "        model.fit(x_train, y_train)\n",
    "        model = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "        models_list.append(model)\n",
    "        \n",
    "    alpha = [0.01] \n",
    "    best_alpha = 999\n",
    "    best_f1_score = -99.99\n",
    "    for i in alpha:\n",
    "        lr = LogisticRegression(C=i)\n",
    "        model = StackingClassifier(classifiers=models_list, meta_classifier=lr, use_probas=True)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict_proba(x_cv)\n",
    "        log_error = log_loss(y_cv, model.predict_proba(x_cv))\n",
    "        print(f\"Stacking Classifer: alpha: {i} Log Loss: {log_error}\")\n",
    "        if best_alpha > log_error:\n",
    "            best_alpha = log_error         \n",
    "            \n",
    "    model = StackingClassifier(classifiers=models_list, meta_classifier=LogisticRegression(C=best_alpha), use_probas=True)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Classification report \n",
    "    print(classification_report(y_test, model.predict(x_test)))\n",
    "    \n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95fc6f7d-f5e8-4fd5-abb5-a77653651311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def votting_classifier(x_train, y_train, x_test, y_test, x_cv, y_cv, *models):\n",
    "    #Refer:http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn.metrics import log_loss, classification_report\n",
    "    \n",
    "    models_list = []\n",
    "    for model in models:\n",
    "        name = str(model).split(\"(\")[0]\n",
    "        model.fit(x_train, y_train)\n",
    "        model = CalibratedClassifierCV(model, method=\"sigmoid\")\n",
    "        models_list.append( tuple((name, model)) )\n",
    "        \n",
    "        \n",
    "    model = VotingClassifier(estimators=models_list, voting='soft')\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # Classification report \n",
    "    print(classification_report(y_test, model.predict(x_test)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83439d5b-ec16-4428-ad68-b60eae1560b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6989, 2000)\n",
      "(2185, 2000)\n",
      "(1748, 2000)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = df['Sentiment'].values\n",
    "X = df['processed_text']\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,  test_size=0.2)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_train = X_train.toarray()\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = vectorizer.transform(X_test)\n",
    "X_test = X_test.toarray()\n",
    "print(X_test.shape)\n",
    "\n",
    "X_cv = vectorizer.transform(X_cv)\n",
    "X_cv = X_cv.toarray()\n",
    "print(X_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b870b5c8-d962-4a50-955f-42f179785adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifer: alpha: 0.01 Log Loss: 0.5774776432718101\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       726\n",
      "           1       0.77      0.77      0.77       649\n",
      "           2       0.73      0.76      0.75       810\n",
      "\n",
      "    accuracy                           0.77      2185\n",
      "   macro avg       0.78      0.77      0.78      2185\n",
      "weighted avg       0.78      0.77      0.77      2185\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/stacking_model.model']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = stacking_models(X_train, y_train, X_test, y_test, X_cv, y_cv,\n",
    "            SGDClassifier(loss='hinge', penalty='l2',n_jobs=-1, alpha=0.0001),  # Linear svm\n",
    "            SVC(kernel='rbf', C=10, gamma=1),  # non-linear svm\n",
    "            MultinomialNB(alpha=0.5),  # nb\n",
    "            LogisticRegression(C=10.0, penalty='l2', solver='lbfgs'), # lr\n",
    "            DecisionTreeClassifier(random_state=1, max_depth=500, min_samples_split=5), # dt\n",
    "            RandomForestClassifier(random_state=1, n_jobs=6, max_depth=150, min_samples_leaf=1, n_estimators=500) # \n",
    "        )\n",
    "\n",
    "joblib.dump(model, 'data/stacking_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c6fed84-0244-4164-a043-741df7c645e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.78      0.81       726\n",
      "           1       0.79      0.73      0.76       649\n",
      "           2       0.70      0.80      0.75       810\n",
      "\n",
      "    accuracy                           0.77      2185\n",
      "   macro avg       0.78      0.77      0.77      2185\n",
      "weighted avg       0.78      0.77      0.77      2185\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/votting_model.model']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = votting_classifier(X_train, y_train, X_test, y_test, X_cv, y_cv,\n",
    "            SGDClassifier(loss='hinge', penalty='l2',n_jobs=-1, alpha= 0.0001),  # Linear\n",
    "            SVC(kernel='rbf', C=10, gamma=1),\n",
    "            MultinomialNB(alpha=0.5),\n",
    "            # KNeighborsClassifier(algorithm='auto', n_neighbors=1),\n",
    "            LogisticRegression(C=10.0, penalty='l2', solver='lbfgs'),\n",
    "            DecisionTreeClassifier(random_state=1, max_depth=500, min_samples_split=5),\n",
    "            RandomForestClassifier(random_state=1, n_jobs=6, max_depth=150, min_samples_leaf=1, n_estimators=500)\n",
    "        )\n",
    "\n",
    "joblib.dump(model, 'data/votting_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d34e8b0e-4ab8-4ec8-acd4-1b457eeb865b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/tfidf_model.model']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = joblib.load('data/oversampled.df')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "joblib.dump(vectorizer, 'data/tfidf_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ed1a04c-e5c9-4f83-a041-be5e207c683b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6989, 2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f52f8-17fa-4084-9d7b-858010cdc4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
